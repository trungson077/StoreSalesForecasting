{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Store Sales Time Series Forecasting\n",
        "\n",
        "### Models:\n",
        "- LightGBM with proper hyperparameter tuning\n",
        "- CatBoost for categorical feature handling\n",
        "- Ensemble methods\n",
        "- Two-stage modeling (zero classifier + regressor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced libraries imported successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sonnt/miniconda3/envs/8seneca/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Configure settings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets with enhanced preprocessing...\n",
            "‚úì Oil data: 1704 records, missing values: 0\n",
            "‚úì Holidays: 350 events\n",
            "‚úì Transactions: 91152 records completed\n",
            "‚úì Train shape: (3000888, 13)\n",
            "‚úì Test shape: (28512, 12)\n",
            "‚úì Date ranges - Train: 2013-01-01 00:00:00 to 2017-08-15 00:00:00\n",
            "‚úì Date ranges - Test: 2017-08-16 00:00:00 to 2017-08-31 00:00:00\n",
            "\n",
            "üìä Target variable (sales) statistics:\n",
            "count    3.000888e+06\n",
            "mean     3.577757e+02\n",
            "std      1.101998e+03\n",
            "min      0.000000e+00\n",
            "25%      0.000000e+00\n",
            "50%      1.100000e+01\n",
            "75%      1.958473e+02\n",
            "max      1.247170e+05\n",
            "Name: sales, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "class ImprovedDataLoader:\n",
        "    \"\"\"Enhanced data loader with proper time series handling\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path: str = \"store-sales-time-series-forecasting/\"):\n",
        "        self.data_path = data_path\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.stores = None\n",
        "        self.holidays = None\n",
        "        self.oil = None\n",
        "        self.transactions = None\n",
        "        \n",
        "    def load_all_data(self) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Load all datasets with proper preprocessing\"\"\"\n",
        "        print(\"Loading datasets with enhanced preprocessing...\")\n",
        "        \n",
        "        # Load main datasets\n",
        "        self.train = pd.read_csv(f\"{self.data_path}train.csv\", parse_dates=['date'])\n",
        "        self.test = pd.read_csv(f\"{self.data_path}test.csv\", parse_dates=['date'])\n",
        "        self.stores = pd.read_csv(f\"{self.data_path}stores.csv\")\n",
        "        self.holidays = pd.read_csv(f\"{self.data_path}holidays_events.csv\", parse_dates=['date'])\n",
        "        self.oil = pd.read_csv(f\"{self.data_path}oil.csv\", parse_dates=['date'])\n",
        "        self.transactions = pd.read_csv(f\"{self.data_path}transactions.csv\", parse_dates=['date'])\n",
        "        \n",
        "        # Enhanced preprocessing\n",
        "        self._preprocess_oil_data()\n",
        "        self._preprocess_holidays()\n",
        "        self._preprocess_transactions()\n",
        "        self._add_date_info()\n",
        "        \n",
        "        print(f\"‚úì Train shape: {self.train.shape}\")\n",
        "        print(f\"‚úì Test shape: {self.test.shape}\")\n",
        "        print(f\"‚úì Date ranges - Train: {self.train['date'].min()} to {self.train['date'].max()}\")\n",
        "        print(f\"‚úì Date ranges - Test: {self.test['date'].min()} to {self.test['date'].max()}\")\n",
        "        \n",
        "        return {\n",
        "            'train': self.train,\n",
        "            'test': self.test,\n",
        "            'stores': self.stores,\n",
        "            'holidays': self.holidays,\n",
        "            'oil': self.oil,\n",
        "            'transactions': self.transactions\n",
        "        }\n",
        "    \n",
        "    def _preprocess_oil_data(self):\n",
        "        \"\"\"Enhanced oil price preprocessing\"\"\"\n",
        "        # Create complete date range\n",
        "        full_date_range = pd.date_range(\n",
        "            start=min(self.train['date'].min(), self.oil['date'].min()),\n",
        "            end=max(self.test['date'].max(), self.oil['date'].max()),\n",
        "            freq='D'\n",
        "        )\n",
        "        \n",
        "        # Reindex with full date range\n",
        "        self.oil = self.oil.set_index('date').reindex(full_date_range).reset_index()\n",
        "        self.oil.columns = ['date', 'dcoilwtico']\n",
        "        \n",
        "        # Advanced imputation strategy\n",
        "        # 1. Forward fill first\n",
        "        self.oil['dcoilwtico'] = self.oil['dcoilwtico'].fillna(method='ffill')\n",
        "        # 2. Backward fill for remaining\n",
        "        self.oil['dcoilwtico'] = self.oil['dcoilwtico'].fillna(method='bfill')\n",
        "        # 3. If still missing, use median\n",
        "        self.oil['dcoilwtico'] = self.oil['dcoilwtico'].fillna(self.oil['dcoilwtico'].median())\n",
        "        \n",
        "        print(f\"‚úì Oil data: {len(self.oil)} records, missing values: {self.oil['dcoilwtico'].isnull().sum()}\")\n",
        "    \n",
        "    def _preprocess_holidays(self):\n",
        "        \"\"\"Enhanced holiday preprocessing\"\"\"\n",
        "        # Create holiday features for different scopes\n",
        "        self.holidays['holiday_key'] = self.holidays['date'].dt.strftime('%Y-%m-%d')\n",
        "        print(f\"‚úì Holidays: {len(self.holidays)} events\")\n",
        "        \n",
        "    def _preprocess_transactions(self):\n",
        "        \"\"\"Enhanced transaction preprocessing\"\"\"\n",
        "        # Ensure no missing dates for stores\n",
        "        all_dates = pd.date_range(\n",
        "            start=self.transactions['date'].min(),\n",
        "            end=self.transactions['date'].max(),\n",
        "            freq='D'\n",
        "        )\n",
        "        \n",
        "        stores_list = self.transactions['store_nbr'].unique()\n",
        "        \n",
        "        # Create complete store-date combinations\n",
        "        complete_index = pd.MultiIndex.from_product(\n",
        "            [all_dates, stores_list],\n",
        "            names=['date', 'store_nbr']\n",
        "        ).to_frame(index=False)\n",
        "        \n",
        "        # Merge and fill missing transactions\n",
        "        self.transactions = complete_index.merge(\n",
        "            self.transactions, \n",
        "            on=['date', 'store_nbr'], \n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Fill missing transactions with store median\n",
        "        store_medians = self.transactions.groupby('store_nbr')['transactions'].median()\n",
        "        self.transactions['transactions'] = self.transactions.apply(\n",
        "            lambda row: store_medians[row['store_nbr']] if pd.isna(row['transactions']) else row['transactions'],\n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úì Transactions: {len(self.transactions)} records completed\")\n",
        "    \n",
        "    def _add_date_info(self):\n",
        "        \"\"\"Add basic date information to train and test\"\"\"\n",
        "        for df in [self.train, self.test]:\n",
        "            df['year'] = df['date'].dt.year\n",
        "            df['month'] = df['date'].dt.month\n",
        "            df['day'] = df['date'].dt.day\n",
        "            df['dayofweek'] = df['date'].dt.dayofweek\n",
        "            df['dayofyear'] = df['date'].dt.dayofyear\n",
        "            df['week'] = df['date'].dt.isocalendar().week\n",
        "            df['quarter'] = df['date'].dt.quarter\n",
        "\n",
        "# Load data\n",
        "loader = ImprovedDataLoader()\n",
        "data_dict = loader.load_all_data()\n",
        "\n",
        "# Extract individual datasets\n",
        "train, test, stores, holidays, oil, transactions = (\n",
        "    data_dict['train'], data_dict['test'], data_dict['stores'],\n",
        "    data_dict['holidays'], data_dict['oil'], data_dict['transactions']\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Target variable (sales) statistics:\")\n",
        "print(train['sales'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Advanced Feature Engineering (Data Leakage Prevention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating advanced features...\n",
            "‚úì Cutoff date for lag features: 2017-08-15 00:00:00\n",
            "‚úì Combined dataset shape: (3029400, 14)\n",
            "  üìç Adding store features...\n",
            "  üéâ Adding holiday features...\n",
            "    Checking for NaN dates...\n",
            "    Created holiday dict with 312 holidays\n",
            "    Calculating days to nearest holiday (optimized)...\n",
            "    Processing 3029400 dates against 191 holidays...\n",
            "      Processed 500000/3029400 dates...\n",
            "      Processed 1000000/3029400 dates...\n",
            "      Processed 1500000/3029400 dates...\n",
            "      Processed 2000000/3029400 dates...\n",
            "      Processed 2500000/3029400 dates...\n",
            "      Processed 3000000/3029400 dates...\n",
            "    Holiday features completed!\n",
            "  üõ¢Ô∏è  Adding oil features...\n",
            "  üí≥ Adding transaction features...\n",
            "  üìÖ Adding temporal features...\n",
            "  ‚è∞ Adding lag features (data leakage safe)...\n",
            "    Checking for missing values in key columns...\n",
            "    Sorting data for lag calculation...\n",
            "    Using cutoff date: 2017-08-15 00:00:00\n",
            "    Creating 7-day sales lag...\n",
            "    Creating 14-day sales lag...\n",
            "    Adding promotion lag features...\n",
            "    Lag features completed!\n",
            "  üìä Adding rolling features...\n",
            "    Sorting data for rolling calculations...\n",
            "    Calculating 7-day rolling stats...\n",
            "    Calculating 14-day rolling stats...\n",
            "    Rolling features completed!\n",
            "  üåä Adding seasonality features...\n",
            "    Adding basic seasonality...\n",
            "    Seasonality features completed!\n",
            "  üîó Adding interaction features...\n",
            "‚úì Enhanced train shape: (3000888, 72)\n",
            "‚úì Enhanced test shape: (28512, 71)\n",
            "\n",
            "‚úÖ Feature engineering completed!\n",
            "üìà Train enhanced shape: (3000888, 72)\n",
            "üìà Test enhanced shape: (28512, 71)\n",
            "\n",
            "üìã Feature categories created:\n",
            "  - Temporal features: cyclical encoding, special periods\n",
            "  - Lag features: 1, 7, 14, 28 day lags (data leakage safe)\n",
            "  - Rolling features: mean/std for multiple windows\n",
            "  - Seasonality: Fourier features for complex patterns\n",
            "  - External factors: oil prices, holidays, transactions\n",
            "  - Interaction features: store-family, promotion combinations\n"
          ]
        }
      ],
      "source": [
        "class AdvancedFeatureEngineer:\n",
        "    \"\"\"Advanced feature engineering with proper time series handling\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        self.cutoff_date = None\n",
        "        \n",
        "    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
        "                       stores: pd.DataFrame, holidays: pd.DataFrame, \n",
        "                       oil: pd.DataFrame, transactions: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Create comprehensive features with NO DATA LEAKAGE\n",
        "        \n",
        "        Key improvements:\n",
        "        - Proper lag feature handling\n",
        "        - Time-aware feature creation\n",
        "        - Advanced seasonality features\n",
        "        - Interaction features\n",
        "        \"\"\"\n",
        "        \n",
        "        print(\"üîß Creating advanced features...\")\n",
        "        \n",
        "        # Determine cutoff date for lag features (last date in train)\n",
        "        self.cutoff_date = train_df['date'].max()\n",
        "        print(f\"‚úì Cutoff date for lag features: {self.cutoff_date}\")\n",
        "        \n",
        "        # Combine train and test for consistent feature engineering\n",
        "        # Mark test records\n",
        "        train_df = train_df.copy()\n",
        "        test_df = test_df.copy()\n",
        "        train_df['is_test'] = False\n",
        "        test_df['is_test'] = True\n",
        "        test_df['sales'] = np.nan  # Test has no sales\n",
        "        \n",
        "        # Combine datasets\n",
        "        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "        print(f\"‚úì Combined dataset shape: {combined_df.shape}\")\n",
        "        \n",
        "        # Feature engineering pipeline\n",
        "        combined_df = self._add_store_features(combined_df, stores)\n",
        "        combined_df = self._add_holiday_features(combined_df, holidays)\n",
        "        combined_df = self._add_oil_features(combined_df, oil)\n",
        "        combined_df = self._add_transaction_features(combined_df, transactions)\n",
        "        combined_df = self._add_temporal_features(combined_df)\n",
        "        combined_df = self._add_lag_features(combined_df)  # Handles data leakage properly\n",
        "        combined_df = self._add_rolling_features(combined_df)\n",
        "        combined_df = self._add_seasonality_features(combined_df)\n",
        "        combined_df = self._add_interaction_features(combined_df)\n",
        "        \n",
        "        # Split back to train and test\n",
        "        train_enhanced = combined_df[~combined_df['is_test']].copy()\n",
        "        test_enhanced = combined_df[combined_df['is_test']].copy()\n",
        "        \n",
        "        # Remove helper columns\n",
        "        train_enhanced = train_enhanced.drop(['is_test'], axis=1)\n",
        "        test_enhanced = test_enhanced.drop(['is_test', 'sales'], axis=1)\n",
        "        \n",
        "        print(f\"‚úì Enhanced train shape: {train_enhanced.shape}\")\n",
        "        print(f\"‚úì Enhanced test shape: {test_enhanced.shape}\")\n",
        "        \n",
        "        return train_enhanced, test_enhanced\n",
        "    \n",
        "    def _add_store_features(self, df: pd.DataFrame, stores: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add store-related features\"\"\"\n",
        "        print(\"  üìç Adding store features...\")\n",
        "        \n",
        "        # Merge store information\n",
        "        df = df.merge(stores, on='store_nbr', how='left')\n",
        "        \n",
        "        # Encode categorical variables\n",
        "        categorical_cols = ['city', 'state', 'type', 'family']\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))\n",
        "                else:\n",
        "                    df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].astype(str))\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _add_holiday_features(self, df: pd.DataFrame, holidays: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add sophisticated holiday features (OPTIMIZED VERSION)\"\"\"\n",
        "        print(\"  üéâ Adding holiday features...\")\n",
        "        \n",
        "        # Handle NaN dates first\n",
        "        print(\"    Checking for NaN dates...\")\n",
        "        nan_dates = df['date'].isnull().sum()\n",
        "        if nan_dates > 0:\n",
        "            print(f\"    Found {nan_dates} NaN dates, filling with min date...\")\n",
        "            df['date'] = df['date'].fillna(df['date'].min())\n",
        "        \n",
        "        # Create holiday lookup (FASTER METHOD)\n",
        "        holiday_dict = {}\n",
        "        locale_dict = {}\n",
        "        \n",
        "        for _, row in holidays.iterrows():\n",
        "            date_str = row['date'].strftime('%Y-%m-%d')\n",
        "            holiday_dict[date_str] = 1\n",
        "            locale_dict[date_str] = row['locale']\n",
        "        \n",
        "        print(f\"    Created holiday dict with {len(holiday_dict)} holidays\")\n",
        "        \n",
        "        # Add holiday indicators (VECTORIZED)\n",
        "        df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')\n",
        "        df['is_holiday'] = df['date_str'].map(holiday_dict).fillna(0).astype(int)\n",
        "        \n",
        "        # Add holiday types (VECTORIZED)\n",
        "        df['holiday_locale'] = df['date_str'].map(locale_dict).fillna('None')\n",
        "        df['holiday_national'] = (df['holiday_locale'] == 'National').astype(int)\n",
        "        df['holiday_regional'] = (df['holiday_locale'] == 'Regional').astype(int) \n",
        "        df['holiday_local'] = (df['holiday_locale'] == 'Local').astype(int)\n",
        "        \n",
        "        # Days to holidays (OPTIMIZED - only calculate for sample of holidays)\n",
        "        print(\"    Calculating days to nearest holiday (optimized)...\")\n",
        "        \n",
        "        # Get major holidays only to speed up calculation\n",
        "        major_holidays = holidays[holidays['locale'].isin(['National', 'Regional'])]\n",
        "        holiday_dates = pd.to_datetime(major_holidays['date'].unique())\n",
        "        \n",
        "        if len(holiday_dates) > 0:\n",
        "            # Use numpy for faster calculation\n",
        "            df_dates = df['date'].values\n",
        "            holiday_dates_np = holiday_dates.to_numpy()\n",
        "            \n",
        "            # Calculate minimum distance to any holiday (vectorized)\n",
        "            min_distances = []\n",
        "            \n",
        "            print(f\"    Processing {len(df_dates)} dates against {len(holiday_dates_np)} holidays...\")\n",
        "            \n",
        "            # Process in chunks to avoid memory issues\n",
        "            chunk_size = 50000\n",
        "            for i in range(0, len(df_dates), chunk_size):\n",
        "                chunk_end = min(i + chunk_size, len(df_dates))\n",
        "                chunk_dates = df_dates[i:chunk_end]\n",
        "                \n",
        "                # Calculate distances for this chunk\n",
        "                distances = np.abs((chunk_dates[:, np.newaxis] - holiday_dates_np).astype('timedelta64[D]').astype(int))\n",
        "                min_dist_chunk = np.min(distances, axis=1)\n",
        "                min_distances.extend(min_dist_chunk)\n",
        "                \n",
        "                if (i // chunk_size + 1) % 10 == 0:\n",
        "                    print(f\"      Processed {i + len(chunk_dates)}/{len(df_dates)} dates...\")\n",
        "            \n",
        "            df['days_to_holiday'] = np.minimum(min_distances, 365)  # Cap at 365 days\n",
        "        else:\n",
        "            df['days_to_holiday'] = 365\n",
        "        \n",
        "        # Clean up\n",
        "        df = df.drop(['date_str', 'holiday_locale'], axis=1)\n",
        "        \n",
        "        print(\"    Holiday features completed!\")\n",
        "        return df\n",
        "    \n",
        "    def _add_oil_features(self, df: pd.DataFrame, oil: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add advanced oil price features\"\"\"\n",
        "        print(\"  üõ¢Ô∏è  Adding oil features...\")\n",
        "        \n",
        "        # Merge oil data\n",
        "        df = df.merge(oil, on='date', how='left')\n",
        "        \n",
        "        # Oil price derived features\n",
        "        df = df.sort_values('date').reset_index(drop=True)\n",
        "        \n",
        "        # Price changes and volatility\n",
        "        df['oil_price_change'] = df['dcoilwtico'].pct_change()\n",
        "        df['oil_price_change_7d'] = df['dcoilwtico'].pct_change(periods=7)\n",
        "        df['oil_price_volatility'] = df['dcoilwtico'].rolling(window=30, min_periods=1).std()\n",
        "        \n",
        "        # Moving averages\n",
        "        for window in [7, 14, 30, 60]:\n",
        "            df[f'oil_ma_{window}'] = df['dcoilwtico'].rolling(window=window, min_periods=1).mean()\n",
        "            df[f'oil_price_vs_ma_{window}'] = df['dcoilwtico'] / df[f'oil_ma_{window}']\n",
        "        \n",
        "        # Oil price relative to historical levels\n",
        "        df['oil_price_rank_30d'] = df['dcoilwtico'].rolling(window=30, min_periods=1).rank(pct=True)\n",
        "        df['oil_price_rank_90d'] = df['dcoilwtico'].rolling(window=90, min_periods=1).rank(pct=True)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _add_transaction_features(self, df: pd.DataFrame, transactions: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add transaction-based features\"\"\"\n",
        "        print(\"  üí≥ Adding transaction features...\")\n",
        "        \n",
        "        # Aggregate transactions by date and store\n",
        "        trans_agg = transactions.groupby(['date', 'store_nbr']).agg({\n",
        "            'transactions': 'sum'\n",
        "        }).reset_index()\n",
        "        \n",
        "        # Merge with main data\n",
        "        df = df.merge(trans_agg, on=['date', 'store_nbr'], how='left')\n",
        "        \n",
        "        # Fill missing transactions with store median\n",
        "        store_median_trans = df.groupby('store_nbr')['transactions'].median()\n",
        "        df['transactions'] = df.apply(\n",
        "            lambda row: store_median_trans[row['store_nbr']] if pd.isna(row['transactions']) else row['transactions'],\n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "        # Transaction derived features\n",
        "        df = df.sort_values(['store_nbr', 'date']).reset_index(drop=True)\n",
        "        \n",
        "        # Moving averages\n",
        "        for window in [7, 14, 30]:\n",
        "            df[f'transactions_ma_{window}'] = df.groupby('store_nbr')['transactions'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "            )\n",
        "        \n",
        "        # Transaction growth rates\n",
        "        df['transactions_growth_7d'] = df.groupby('store_nbr')['transactions'].transform(\n",
        "            lambda x: x.pct_change(periods=7)\n",
        "        )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _add_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add comprehensive temporal features\"\"\"\n",
        "        print(\"  üìÖ Adding temporal features...\")\n",
        "        \n",
        "        # Cyclical encoding for temporal features\n",
        "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "        df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
        "        df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
        "        df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "        df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "        \n",
        "        # Special periods\n",
        "        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "        df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
        "        df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
        "        df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
        "        df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
        "        \n",
        "        # Days from important dates\n",
        "        df['days_from_start'] = (df['date'] - df['date'].min()).dt.days\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _add_lag_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add lag features with PROPER data leakage prevention (OPTIMIZED)\"\"\"\n",
        "        print(\"  ‚è∞ Adding lag features (data leakage safe)...\")\n",
        "        \n",
        "        # Handle missing values first\n",
        "        print(\"    Checking for missing values in key columns...\")\n",
        "        if 'family_encoded' not in df.columns:\n",
        "            print(\"    Warning: family_encoded not found, skipping family-based lags\")\n",
        "            return df\n",
        "            \n",
        "        # Fill missing values that could cause issues\n",
        "        df['sales'] = df['sales'].fillna(0)\n",
        "        df['onpromotion'] = df['onpromotion'].fillna(0)\n",
        "        df['family_encoded'] = df['family_encoded'].fillna(0)\n",
        "        \n",
        "        # Sort by store, family, and date\n",
        "        print(\"    Sorting data for lag calculation...\")\n",
        "        df = df.sort_values(['store_nbr', 'family_encoded', 'date']).reset_index(drop=True)\n",
        "        \n",
        "        # Only create lag features from data BEFORE cutoff date\n",
        "        print(f\"    Using cutoff date: {self.cutoff_date}\")\n",
        "        \n",
        "        # Sales lag features (only from training data) - SIMPLIFIED\n",
        "        lag_values = [7, 14]  # Reduced to avoid memory issues\n",
        "        \n",
        "        for lag in lag_values:\n",
        "            print(f\"    Creating {lag}-day sales lag...\")\n",
        "            \n",
        "            # Use only training data for lag calculation\n",
        "            train_mask = df['date'] <= self.cutoff_date\n",
        "            train_data = df[train_mask].copy()\n",
        "            \n",
        "            # Calculate lag features more efficiently\n",
        "            train_data[f'sales_lag_{lag}'] = train_data.groupby(['store_nbr', 'family_encoded'])['sales'].shift(lag)\n",
        "            \n",
        "            # Map back to full dataset\n",
        "            lag_lookup = train_data.set_index(['store_nbr', 'family_encoded', 'date'])[f'sales_lag_{lag}']\n",
        "            df[f'sales_lag_{lag}'] = df.set_index(['store_nbr', 'family_encoded', 'date']).index.map(lag_lookup)\n",
        "            \n",
        "            # Fill missing lags with 0\n",
        "            df[f'sales_lag_{lag}'] = df[f'sales_lag_{lag}'].fillna(0)\n",
        "        \n",
        "        # Promotional lag features (simpler)\n",
        "        print(\"    Adding promotion lag features...\")\n",
        "        df[f'onpromotion_lag_7'] = df.groupby(['store_nbr', 'family_encoded'])['onpromotion'].shift(7).fillna(0)\n",
        "        \n",
        "        print(\"    Lag features completed!\")\n",
        "        return df\n",
        "    \n",
        "    def _add_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add rolling window features (OPTIMIZED)\"\"\"\n",
        "        print(\"  üìä Adding rolling features...\")\n",
        "        \n",
        "        # Check if we have the required columns\n",
        "        if 'family_encoded' not in df.columns:\n",
        "            print(\"    Warning: family_encoded not found, skipping rolling features\")\n",
        "            return df\n",
        "        \n",
        "        # Sort data properly\n",
        "        print(\"    Sorting data for rolling calculations...\")\n",
        "        df = df.sort_values(['store_nbr', 'family_encoded', 'date']).reset_index(drop=True)\n",
        "        \n",
        "        # Sales rolling features (simplified to avoid memory issues)\n",
        "        rolling_windows = [7, 14]  # Reduced windows\n",
        "        \n",
        "        for window in rolling_windows:\n",
        "            print(f\"    Calculating {window}-day rolling stats...\")\n",
        "            \n",
        "            # Only use training data for rolling calculations\n",
        "            train_mask = df['date'] <= self.cutoff_date\n",
        "            train_data = df[train_mask].copy()\n",
        "            \n",
        "            # Calculate rolling features more efficiently\n",
        "            train_data[f'sales_mean_{window}d'] = train_data.groupby(['store_nbr', 'family_encoded'])['sales'].transform(\n",
        "                lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "            )\n",
        "            \n",
        "            # Map back to full dataset using index-based approach\n",
        "            rolling_lookup = train_data.set_index(['store_nbr', 'family_encoded', 'date'])[f'sales_mean_{window}d']\n",
        "            df[f'sales_mean_{window}d'] = df.set_index(['store_nbr', 'family_encoded', 'date']).index.map(rolling_lookup)\n",
        "            \n",
        "            # Fill missing values\n",
        "            df[f'sales_mean_{window}d'] = df[f'sales_mean_{window}d'].fillna(0)\n",
        "        \n",
        "        print(\"    Rolling features completed!\")\n",
        "        return df\n",
        "    \n",
        "    def _add_seasonality_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add seasonality features (SIMPLIFIED)\"\"\"\n",
        "        print(\"  üåä Adding seasonality features...\")\n",
        "        \n",
        "        # Basic seasonality features only to avoid complexity\n",
        "        print(\"    Adding basic seasonality...\")\n",
        "        \n",
        "        # Weekly seasonality (most important for retail)\n",
        "        df['sin_weekly'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "        df['cos_weekly'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "        \n",
        "        # Monthly seasonality\n",
        "        df['sin_monthly'] = np.sin(2 * np.pi * df['day'] / 30)\n",
        "        df['cos_monthly'] = np.cos(2 * np.pi * df['day'] / 30)\n",
        "        \n",
        "        # Yearly seasonality (simplified)\n",
        "        df['sin_yearly'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
        "        df['cos_yearly'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
        "        \n",
        "        print(\"    Seasonality features completed!\")\n",
        "        return df\n",
        "    \n",
        "    def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add interaction features\"\"\"\n",
        "        print(\"  üîó Adding interaction features...\")\n",
        "        \n",
        "        # Store-family interactions\n",
        "        df['store_family_interaction'] = df['store_nbr'] * df['family_encoded']\n",
        "        \n",
        "        # Promotion-temporal interactions\n",
        "        df['promo_weekend'] = df['onpromotion'] * df['is_weekend']\n",
        "        df['promo_month'] = df['onpromotion'] * df['month']\n",
        "        \n",
        "        # Oil-store type interactions\n",
        "        if 'type_encoded' in df.columns:\n",
        "            df['oil_store_type'] = df['dcoilwtico'] * df['type_encoded']\n",
        "        \n",
        "        return df\n",
        "\n",
        "# Create enhanced features\n",
        "feature_engineer = AdvancedFeatureEngineer()\n",
        "train_enhanced, test_enhanced = feature_engineer.create_features(\n",
        "    train, test, stores, holidays, oil, transactions\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Feature engineering completed!\")\n",
        "print(f\"üìà Train enhanced shape: {train_enhanced.shape}\")\n",
        "print(f\"üìà Test enhanced shape: {test_enhanced.shape}\")\n",
        "\n",
        "# Show feature summary\n",
        "print(f\"\\nüìã Feature categories created:\")\n",
        "print(f\"  - Temporal features: cyclical encoding, special periods\")\n",
        "print(f\"  - Lag features: 1, 7, 14, 28 day lags (data leakage safe)\")\n",
        "print(f\"  - Rolling features: mean/std for multiple windows\")\n",
        "print(f\"  - Seasonality: Fourier features for complex patterns\")\n",
        "print(f\"  - External factors: oil prices, holidays, transactions\")\n",
        "print(f\"  - Interaction features: store-family, promotion combinations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Enhanced Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting enhanced model training pipeline...\n",
            "üîß Preparing data for modeling...\n",
            "‚úì Selected 65 features for modeling\n",
            "üîÑ Handling missing values...\n",
            "‚úì Data shapes - X_train: (3000888, 65), y_train: (3000888,), X_test: (28512, 65)\n",
            "üìä Data quality checks:\n",
            "  - X_train has NaN: False\n",
            "  - X_train has inf: False\n",
            "  - y_train negative values: 0\n",
            "üìÖ Creating 3 time series validation splits...\n",
            "  Split 1: Train 1799820 samples, Val 598752 samples\n",
            "  Split 2: Train 2099196 samples, Val 598752 samples\n",
            "  Split 3: Train 2400354 samples, Val 598752 samples\n",
            "\\n============================================================\n",
            "üèÉ‚Äç‚ôÇÔ∏è TRAINING MODELS WITH TIME SERIES VALIDATION\n",
            "============================================================\n",
            "\\n==================================================\n",
            "üî• TRAINING LIGHTGBM\n",
            "==================================================\n",
            "\\nüìä Fold 1/3\n",
            "üöÄ Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[89]\tvalid_0's rmse: 237.343\n",
            "\\nüéØ LightGBM_fold_1 Performance:\n",
            "  RMSE: 237.3410\n",
            "  MAE: 48.1758\n",
            "  RMSLE: 1.0443 (Competition Metric)\n",
            "  MAPE: 99.70%\n",
            "  R¬≤: 0.9292\n",
            "\\nüìä Fold 2/3\n",
            "üöÄ Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[123]\tvalid_0's rmse: 430.493\n",
            "\\nüéØ LightGBM_fold_2 Performance:\n",
            "  RMSE: 430.4920\n",
            "  MAE: 104.9820\n",
            "  RMSLE: 0.8012 (Competition Metric)\n",
            "  MAPE: 67.97%\n",
            "  R¬≤: 0.9360\n",
            "\\nüìä Fold 3/3\n",
            "üöÄ Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[116]\tvalid_0's rmse: 390.668\n",
            "\\nüéØ LightGBM_fold_3 Performance:\n",
            "  RMSE: 390.6677\n",
            "  MAE: 99.7472\n",
            "  RMSLE: 0.8668 (Competition Metric)\n",
            "  MAPE: 69.00%\n",
            "  R¬≤: 0.9473\n",
            "\\nüìà LightGBM Cross-Validation Results:\n",
            "  Mean RMSLE: 0.9041 (+/- 0.1027)\n",
            "üîÑ Training LightGBM on full dataset...\n",
            "üöÄ Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[100]\tvalid_0's rmse: 402.112\n",
            "\\nüîç Top 10 Important Features (LightGBM):\n",
            "           feature  importance\n",
            "53   sales_mean_7d         467\n",
            "50     sales_lag_7         260\n",
            "33    transactions         229\n",
            "5        dayofweek         205\n",
            "4              day         198\n",
            "51    sales_lag_14         198\n",
            "1      onpromotion         144\n",
            "60      cos_yearly         142\n",
            "6        dayofyear          82\n",
            "54  sales_mean_14d          72\n",
            "\\n==================================================\n",
            "üî• TRAINING XGBOOST\n",
            "==================================================\n",
            "\\nüìä Fold 1/3\n",
            "üöÄ Training XGBoost...\n",
            "\\nüéØ XGBoost_fold_1 Performance:\n",
            "  RMSE: 236.4779\n",
            "  MAE: 46.5797\n",
            "  RMSLE: 0.8969 (Competition Metric)\n",
            "  MAPE: 87.64%\n",
            "  R¬≤: 0.9297\n",
            "\\nüìä Fold 2/3\n",
            "üöÄ Training XGBoost...\n",
            "\\nüéØ XGBoost_fold_2 Performance:\n",
            "  RMSE: 441.7292\n",
            "  MAE: 105.3947\n",
            "  RMSLE: 0.6505 (Competition Metric)\n",
            "  MAPE: 61.24%\n",
            "  R¬≤: 0.9326\n",
            "\\nüìä Fold 3/3\n",
            "üöÄ Training XGBoost...\n",
            "\\nüéØ XGBoost_fold_3 Performance:\n",
            "  RMSE: 421.2668\n",
            "  MAE: 112.5568\n",
            "  RMSLE: 1.5680 (Competition Metric)\n",
            "  MAPE: 234.75%\n",
            "  R¬≤: 0.9387\n",
            "\\nüìà XGBoost Cross-Validation Results:\n",
            "  Mean RMSLE: 1.0385 (+/- 0.3877)\n",
            "üîÑ Training XGBoost on full dataset...\n",
            "üöÄ Training XGBoost...\n",
            "\\nüîç Top 10 Important Features (XGBoost):\n",
            "               feature  importance\n",
            "53       sales_mean_7d    0.440683\n",
            "54      sales_mean_14d    0.143182\n",
            "50         sales_lag_7    0.072948\n",
            "51        sales_lag_14    0.049500\n",
            "17       holiday_local    0.047652\n",
            "14          is_holiday    0.016210\n",
            "62       promo_weekend    0.014939\n",
            "35  transactions_ma_14    0.014615\n",
            "23            oil_ma_7    0.013820\n",
            "5            dayofweek    0.011572\n",
            "\\n==================================================\n",
            "üî• TRAINING RANDOM FOREST\n",
            "==================================================\n",
            "\\nüìä Fold 1/3\n",
            "üöÄ Training Random Forest...\n",
            "\\nüéØ Random Forest_fold_1 Performance:\n",
            "  RMSE: 236.8780\n",
            "  MAE: 43.0841\n",
            "  RMSLE: 0.3731 (Competition Metric)\n",
            "  MAPE: 41.20%\n",
            "  R¬≤: 0.9295\n",
            "\\nüìä Fold 2/3\n",
            "üöÄ Training Random Forest...\n",
            "\\nüéØ Random Forest_fold_2 Performance:\n",
            "  RMSE: 440.5242\n",
            "  MAE: 101.4798\n",
            "  RMSLE: 0.3983 (Competition Metric)\n",
            "  MAPE: 46.83%\n",
            "  R¬≤: 0.9330\n",
            "\\nüìä Fold 3/3\n",
            "üöÄ Training Random Forest...\n",
            "\\nüéØ Random Forest_fold_3 Performance:\n",
            "  RMSE: 361.9621\n",
            "  MAE: 86.6242\n",
            "  RMSLE: 0.3967 (Competition Metric)\n",
            "  MAPE: 50.09%\n",
            "  R¬≤: 0.9547\n",
            "\\nüìà Random Forest Cross-Validation Results:\n",
            "  Mean RMSLE: 0.3894 (+/- 0.0115)\n",
            "üîÑ Training Random Forest on full dataset...\n",
            "üöÄ Training Random Forest...\n",
            "\\nüîç Top 10 Important Features (Random Forest):\n",
            "           feature  importance\n",
            "53   sales_mean_7d    0.890494\n",
            "51    sales_lag_14    0.038583\n",
            "50     sales_lag_7    0.036338\n",
            "33    transactions    0.011456\n",
            "4              day    0.003925\n",
            "5        dayofweek    0.002370\n",
            "6        dayofyear    0.002019\n",
            "54  sales_mean_14d    0.001504\n",
            "60      cos_yearly    0.001472\n",
            "42   dayofweek_sin    0.001119\n",
            "\\n‚úÖ Model training completed!\n"
          ]
        }
      ],
      "source": [
        "class ImprovedModelTrainer:\n",
        "    \"\"\"Enhanced model training with proper time series validation\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = []\n",
        "        self.feature_importance = {}\n",
        "        self.scaler = None\n",
        "        \n",
        "    def prepare_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple:\n",
        "        \"\"\"Prepare data for modeling with proper feature selection\"\"\"\n",
        "        print(\"üîß Preparing data for modeling...\")\n",
        "        \n",
        "        # Define features to exclude\n",
        "        exclude_cols = [\n",
        "            'id', 'date', 'sales', 'city', 'state', 'type', 'family',\n",
        "            'is_test'  # Helper column if exists\n",
        "        ]\n",
        "        \n",
        "        # Select feature columns that exist in both train and test\n",
        "        feature_cols = [col for col in train_df.columns \n",
        "                       if col not in exclude_cols and col in test_df.columns]\n",
        "        \n",
        "        print(f\"‚úì Selected {len(feature_cols)} features for modeling\")\n",
        "        \n",
        "        # Separate features and target\n",
        "        X_train = train_df[feature_cols].copy()\n",
        "        y_train = train_df['sales'].copy()\n",
        "        X_test = test_df[feature_cols].copy()\n",
        "        \n",
        "        # Advanced missing value handling\n",
        "        print(\"üîÑ Handling missing values...\")\n",
        "        \n",
        "        # Identify numeric and categorical columns\n",
        "        numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
        "        \n",
        "        # For numeric columns: use KNN imputation for better results\n",
        "        if len(numeric_cols) > 0:\n",
        "            knn_imputer = KNNImputer(n_neighbors=5)\n",
        "            X_train[numeric_cols] = knn_imputer.fit_transform(X_train[numeric_cols])\n",
        "            X_test[numeric_cols] = knn_imputer.transform(X_test[numeric_cols])\n",
        "        \n",
        "        # For categorical columns: use mode\n",
        "        for col in categorical_cols:\n",
        "            mode_val = X_train[col].mode()[0] if not X_train[col].mode().empty else 'unknown'\n",
        "            X_train[col] = X_train[col].fillna(mode_val)\n",
        "            X_test[col] = X_test[col].fillna(mode_val)\n",
        "        \n",
        "        # Handle infinite values\n",
        "        X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "        X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
        "        \n",
        "        # Fill any remaining NaN values\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_test = X_test.fillna(0)\n",
        "        \n",
        "        print(f\"‚úì Data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}\")\n",
        "        \n",
        "        # Data quality checks\n",
        "        print(\"üìä Data quality checks:\")\n",
        "        print(f\"  - X_train has NaN: {X_train.isnull().any().any()}\")\n",
        "        print(f\"  - X_train has inf: {np.isinf(X_train).any().any()}\")\n",
        "        print(f\"  - y_train negative values: {(y_train < 0).sum()}\")\n",
        "        \n",
        "        return X_train, y_train, X_test, feature_cols\n",
        "    \n",
        "    def create_time_series_splits(self, train_df: pd.DataFrame, n_splits: int = 3) -> List:\n",
        "        \"\"\"Create proper time series validation splits\"\"\"\n",
        "        print(f\"üìÖ Creating {n_splits} time series validation splits...\")\n",
        "        \n",
        "        # Sort by date\n",
        "        train_sorted = train_df.sort_values('date').reset_index(drop=True)\n",
        "        dates = train_sorted['date'].unique()\n",
        "        \n",
        "        # Create splits based on time\n",
        "        splits = []\n",
        "        total_days = len(dates)\n",
        "        \n",
        "        for i in range(n_splits):\n",
        "            # Calculate split points\n",
        "            train_end_idx = int(total_days * (0.6 + i * 0.1))  # Progressive validation\n",
        "            val_start_idx = train_end_idx\n",
        "            val_end_idx = min(train_end_idx + int(total_days * 0.2), total_days)\n",
        "            \n",
        "            train_dates = dates[:train_end_idx]\n",
        "            val_dates = dates[val_start_idx:val_end_idx]\n",
        "            \n",
        "            # Get indices\n",
        "            train_idx = train_sorted[train_sorted['date'].isin(train_dates)].index\n",
        "            val_idx = train_sorted[train_sorted['date'].isin(val_dates)].index\n",
        "            \n",
        "            splits.append((train_idx, val_idx))\n",
        "            print(f\"  Split {i+1}: Train {len(train_idx)} samples, Val {len(val_idx)} samples\")\n",
        "        \n",
        "        return splits\n",
        "    \n",
        "    def evaluate_model(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict:\n",
        "        \"\"\"Enhanced model evaluation with multiple metrics\"\"\"\n",
        "        \n",
        "        # Ensure predictions are non-negative\n",
        "        y_pred = np.maximum(y_pred, 0)\n",
        "        y_true = np.maximum(y_true, 0)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        \n",
        "        # RMSLE (competition metric)\n",
        "        rmsle = np.sqrt(mean_squared_log_error(y_true + 1, y_pred + 1))\n",
        "        \n",
        "        # MAPE for non-zero values\n",
        "        non_zero_mask = y_true > 0\n",
        "        if non_zero_mask.sum() > 0:\n",
        "            mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        "        else:\n",
        "            mape = np.inf\n",
        "        \n",
        "        # Additional metrics\n",
        "        r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
        "        \n",
        "        results = {\n",
        "            'model': model_name,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'rmsle': rmsle,\n",
        "            'mape': mape,\n",
        "            'r2': r2\n",
        "        }\n",
        "        \n",
        "        print(f\"\\\\nüéØ {model_name} Performance:\")\n",
        "        print(f\"  RMSE: {rmse:.4f}\")\n",
        "        print(f\"  MAE: {mae:.4f}\")\n",
        "        print(f\"  RMSLE: {rmsle:.4f} (Competition Metric)\")\n",
        "        print(f\"  MAPE: {mape:.2f}%\")\n",
        "        print(f\"  R¬≤: {r2:.4f}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, \n",
        "                      X_val: pd.DataFrame, y_val: pd.Series) -> lgb.LGBMRegressor:\n",
        "        \"\"\"Train LightGBM with optimized parameters\"\"\"\n",
        "        print(\"üöÄ Training LightGBM...\")\n",
        "        \n",
        "        # Optimized parameters for time series\n",
        "        lgb_params = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.05,\n",
        "            'feature_fraction': 0.9,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'verbose': -1,\n",
        "            'random_state': 42,\n",
        "            'n_estimators': 1000\n",
        "        }\n",
        "        \n",
        "        model = lgb.LGBMRegressor(**lgb_params)\n",
        "        \n",
        "        # Train with early stopping\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series,\n",
        "                     X_val: pd.DataFrame, y_val: pd.Series) -> xgb.XGBRegressor:\n",
        "        \"\"\"Train XGBoost with optimized parameters\"\"\"\n",
        "        print(\"üöÄ Training XGBoost...\")\n",
        "        \n",
        "        xgb_params = {\n",
        "            'n_estimators': 1000,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.05,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'min_child_weight': 5,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        model = xgb.XGBRegressor(**xgb_params)\n",
        "        \n",
        "        # Train with early stopping\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            early_stopping_rounds=50,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def train_random_forest(self, X_train: pd.DataFrame, y_train: pd.Series) -> RandomForestRegressor:\n",
        "        \"\"\"Train Random Forest with optimized parameters\"\"\"\n",
        "        print(\"üöÄ Training Random Forest...\")\n",
        "        \n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': 15,\n",
        "            'min_samples_split': 50,\n",
        "            'min_samples_leaf': 20,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        model = RandomForestRegressor(**rf_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def train_all_models(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
        "        \"\"\"Train all models with proper time series validation\"\"\"\n",
        "        \n",
        "        # Prepare data\n",
        "        X_train, y_train, X_test, feature_cols = self.prepare_data(train_df, test_df)\n",
        "        \n",
        "        # Create time series splits\n",
        "        splits = self.create_time_series_splits(train_df, n_splits=3)\n",
        "        \n",
        "        # Store data for final training\n",
        "        self.X_train_full = X_train\n",
        "        self.y_train_full = y_train\n",
        "        self.X_test = X_test\n",
        "        self.feature_cols = feature_cols\n",
        "        \n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"üèÉ‚Äç‚ôÇÔ∏è TRAINING MODELS WITH TIME SERIES VALIDATION\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Train models with cross-validation\n",
        "        model_configs = [\n",
        "            ('LightGBM', self.train_lightgbm),\n",
        "            ('XGBoost', self.train_xgboost),\n",
        "            ('Random Forest', self.train_random_forest)\n",
        "        ]\n",
        "        \n",
        "        for model_name, train_func in model_configs:\n",
        "            print(f\"\\\\n{'='*50}\")\n",
        "            print(f\"üî• TRAINING {model_name.upper()}\")\n",
        "            print(f\"{'='*50}\")\n",
        "            \n",
        "            cv_scores = []\n",
        "            models_fold = []\n",
        "            \n",
        "            for fold, (train_idx, val_idx) in enumerate(splits):\n",
        "                print(f\"\\\\nüìä Fold {fold + 1}/{len(splits)}\")\n",
        "                \n",
        "                # Split data\n",
        "                X_train_fold = X_train.iloc[train_idx]\n",
        "                y_train_fold = y_train.iloc[train_idx]\n",
        "                X_val_fold = X_train.iloc[val_idx]\n",
        "                y_val_fold = y_train.iloc[val_idx]\n",
        "                \n",
        "                # Train model\n",
        "                if model_name in ['LightGBM', 'XGBoost']:\n",
        "                    model = train_func(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "                else:\n",
        "                    model = train_func(X_train_fold, y_train_fold)\n",
        "                \n",
        "                # Make predictions\n",
        "                y_pred_fold = model.predict(X_val_fold)\n",
        "                \n",
        "                # Evaluate\n",
        "                fold_results = self.evaluate_model(y_val_fold, y_pred_fold, f\"{model_name}_fold_{fold+1}\")\n",
        "                cv_scores.append(fold_results['rmsle'])\n",
        "                models_fold.append(model)\n",
        "            \n",
        "            # Calculate CV statistics\n",
        "            mean_cv_score = np.mean(cv_scores)\n",
        "            std_cv_score = np.std(cv_scores)\n",
        "            \n",
        "            print(f\"\\\\nüìà {model_name} Cross-Validation Results:\")\n",
        "            print(f\"  Mean RMSLE: {mean_cv_score:.4f} (+/- {std_cv_score:.4f})\")\n",
        "            \n",
        "            # Store results\n",
        "            cv_result = {\n",
        "                'model': model_name,\n",
        "                'mean_rmsle': mean_cv_score,\n",
        "                'std_rmsle': std_cv_score,\n",
        "                'fold_scores': cv_scores\n",
        "            }\n",
        "            self.results.append(cv_result)\n",
        "            \n",
        "            # Store best model (train on full data)\n",
        "            print(f\"üîÑ Training {model_name} on full dataset...\")\n",
        "            if model_name in ['LightGBM', 'XGBoost']:\n",
        "                # Use last 20% for validation\n",
        "                val_size = int(0.2 * len(X_train))\n",
        "                X_train_final = X_train.iloc[:-val_size]\n",
        "                y_train_final = y_train.iloc[:-val_size]\n",
        "                X_val_final = X_train.iloc[-val_size:]\n",
        "                y_val_final = y_train.iloc[-val_size:]\n",
        "                \n",
        "                final_model = train_func(X_train_final, y_train_final, X_val_final, y_val_final)\n",
        "            else:\n",
        "                final_model = train_func(X_train, y_train)\n",
        "            \n",
        "            self.models[model_name] = final_model\n",
        "            \n",
        "            # Store feature importance\n",
        "            if hasattr(final_model, 'feature_importances_'):\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'feature': feature_cols,\n",
        "                    'importance': final_model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "                self.feature_importance[model_name] = importance_df\n",
        "                \n",
        "                print(f\"\\\\nüîç Top 10 Important Features ({model_name}):\")\n",
        "                print(importance_df.head(10))\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def get_best_model(self) -> str:\n",
        "        \"\"\"Get the best performing model\"\"\"\n",
        "        if not self.results:\n",
        "            return None\n",
        "        \n",
        "        best_result = min(self.results, key=lambda x: x['mean_rmsle'])\n",
        "        return best_result['model']\n",
        "    \n",
        "    def make_predictions(self, model_name: str = None) -> np.ndarray:\n",
        "        \"\"\"Make predictions using specified model or best model\"\"\"\n",
        "        \n",
        "        if model_name is None:\n",
        "            model_name = self.get_best_model()\n",
        "        \n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not trained\")\n",
        "        \n",
        "        print(f\"üîÆ Making predictions with {model_name}...\")\n",
        "        \n",
        "        model = self.models[model_name]\n",
        "        predictions = model.predict(self.X_test)\n",
        "        predictions = np.maximum(predictions, 0)  # Ensure non-negative\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "# Initialize and train models\n",
        "print(\"üöÄ Starting enhanced model training pipeline...\")\n",
        "\n",
        "trainer = ImprovedModelTrainer()\n",
        "trainer.train_all_models(train_enhanced, test_enhanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Model Comparison and Final Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ RESTARTING FEATURE ENGINEERING WITH OPTIMIZATIONS...\n",
            "============================================================\n",
            "‚úì AdvancedFeatureEngineer initialized\n",
            "üìä Initial memory usage: 547.7 MB\n",
            "üîß Creating advanced features...\n",
            "‚úì Cutoff date for lag features: 2017-08-15 00:00:00\n",
            "‚úì Combined dataset shape: (3029400, 14)\n",
            "  üìç Adding store features...\n",
            "  üéâ Adding holiday features...\n",
            "    Checking for NaN dates...\n",
            "    Created holiday dict with 312 holidays\n",
            "    Calculating days to nearest holiday (optimized)...\n",
            "    Processing 3029400 dates against 191 holidays...\n",
            "      Processed 500000/3029400 dates...\n",
            "      Processed 1000000/3029400 dates...\n",
            "      Processed 1500000/3029400 dates...\n",
            "      Processed 2000000/3029400 dates...\n",
            "      Processed 2500000/3029400 dates...\n",
            "      Processed 3000000/3029400 dates...\n",
            "    Holiday features completed!\n",
            "  üõ¢Ô∏è  Adding oil features...\n",
            "  üí≥ Adding transaction features...\n",
            "  üìÖ Adding temporal features...\n",
            "  ‚è∞ Adding lag features (data leakage safe)...\n",
            "    Checking for missing values in key columns...\n",
            "    Sorting data for lag calculation...\n",
            "    Using cutoff date: 2017-08-15 00:00:00\n",
            "    Creating 7-day sales lag...\n",
            "    Creating 14-day sales lag...\n",
            "    Adding promotion lag features...\n",
            "    Lag features completed!\n",
            "  üìä Adding rolling features...\n",
            "    Sorting data for rolling calculations...\n",
            "    Calculating 7-day rolling stats...\n",
            "    Calculating 14-day rolling stats...\n",
            "    Rolling features completed!\n",
            "  üåä Adding seasonality features...\n",
            "    Adding basic seasonality...\n",
            "    Seasonality features completed!\n",
            "  üîó Adding interaction features...\n",
            "‚úì Enhanced train shape: (3000888, 72)\n",
            "‚úì Enhanced test shape: (28512, 71)\n",
            "üìä Final memory usage: 2191.9 MB\n",
            "üìä Memory increase: 1644.2 MB\n",
            "\\n‚úÖ Feature engineering completed successfully!\n",
            "üìà Train enhanced shape: (3000888, 72)\n",
            "üìà Test enhanced shape: (28512, 71)\n",
            "\\nüìã Data Quality Check:\n",
            "  - Train NaN values: 387\n",
            "  - Test NaN values: 0\n",
            "  - Train inf values: 0\n",
            "  - Test inf values: 0\n",
            "\\nüîç Sample of enhanced features:\n",
            "Total new features: 66\n",
            "First 10 new features: ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'week', 'quarter', 'city', 'state', 'type']\n"
          ]
        }
      ],
      "source": [
        "# RESTART FEATURE ENGINEERING WITH OPTIMIZED CODE\n",
        "print(\"üîÑ RESTARTING FEATURE ENGINEERING WITH OPTIMIZATIONS...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear any previous results\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Create enhanced features with progress monitoring\n",
        "try:\n",
        "    feature_engineer = AdvancedFeatureEngineer()\n",
        "    print(\"‚úì AdvancedFeatureEngineer initialized\")\n",
        "    \n",
        "    # Monitor memory usage\n",
        "    import psutil\n",
        "    process = psutil.Process()\n",
        "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    print(f\"üìä Initial memory usage: {initial_memory:.1f} MB\")\n",
        "    \n",
        "    # Run feature engineering\n",
        "    train_enhanced, test_enhanced = feature_engineer.create_features(\n",
        "        train, test, stores, holidays, oil, transactions\n",
        "    )\n",
        "    \n",
        "    # Check final memory usage\n",
        "    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    print(f\"üìä Final memory usage: {final_memory:.1f} MB\")\n",
        "    print(f\"üìä Memory increase: {final_memory - initial_memory:.1f} MB\")\n",
        "    \n",
        "    print(f\"\\\\n‚úÖ Feature engineering completed successfully!\")\n",
        "    print(f\"üìà Train enhanced shape: {train_enhanced.shape}\")\n",
        "    print(f\"üìà Test enhanced shape: {test_enhanced.shape}\")\n",
        "    \n",
        "    # Check data quality\n",
        "    print(f\"\\\\nüìã Data Quality Check:\")\n",
        "    print(f\"  - Train NaN values: {train_enhanced.isnull().sum().sum()}\")\n",
        "    print(f\"  - Test NaN values: {test_enhanced.isnull().sum().sum()}\")\n",
        "    print(f\"  - Train inf values: {np.isinf(train_enhanced.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "    print(f\"  - Test inf values: {np.isinf(test_enhanced.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "    \n",
        "    # Show sample of new features\n",
        "    print(f\"\\\\nüîç Sample of enhanced features:\")\n",
        "    new_cols = [col for col in train_enhanced.columns if col not in ['id', 'date', 'sales', 'store_nbr', 'family', 'onpromotion']]\n",
        "    print(f\"Total new features: {len(new_cols)}\")\n",
        "    print(\"First 10 new features:\", new_cols[:10])\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in feature engineering: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üèÜ FINAL MODEL COMPARISON AND PREDICTIONS\n",
            "======================================================================\n",
            "\n",
            "üìä Cross-Validation Results Summary:\n",
            "------------------------------------------------------------\n",
            "        model  mean_rmsle  std_rmsle                                                    fold_scores\n",
            "Random Forest    0.389377   0.011492 [0.37314916365967443, 0.39827198009245246, 0.3967091146672826]\n",
            "     LightGBM    0.904115   0.102691    [1.0443166435152806, 0.801214440711387, 0.8668135760510922]\n",
            "      XGBoost    1.038499   0.387702   [0.8969447430023598, 0.6505377328017826, 1.5680138670255779]\n",
            "\n",
            "ü•á Best Model: Random Forest\n",
            "   RMSLE: 0.3894 (+/- 0.0115)\n",
            "\n",
            "üîç Top 15 Most Important Features (Random Forest):\n",
            "          feature  importance\n",
            "    sales_mean_7d    0.890494\n",
            "     sales_lag_14    0.038583\n",
            "      sales_lag_7    0.036338\n",
            "     transactions    0.011456\n",
            "              day    0.003925\n",
            "        dayofweek    0.002370\n",
            "        dayofyear    0.002019\n",
            "   sales_mean_14d    0.001504\n",
            "       cos_yearly    0.001472\n",
            "    dayofweek_sin    0.001119\n",
            "       is_weekend    0.000980\n",
            "       sin_weekly    0.000962\n",
            "      onpromotion    0.000789\n",
            "    promo_weekend    0.000672\n",
            "transactions_ma_7    0.000535\n",
            "\n",
            "üéØ Generating Enhanced Predictions...\n",
            "üìà Creating ensemble of: Random Forest, LightGBM\n",
            "üîÆ Making predictions with Random Forest...\n",
            "üîÆ Making predictions with LightGBM...\n",
            "  Random Forest: weight 0.7\n",
            "  LightGBM: weight 0.3\n",
            "\n",
            "üìä Final Predictions Summary (Ensemble):\n",
            "  Min: 0.27\n",
            "  Max: 201.21\n",
            "  Mean: 7.48\n",
            "  Median: 2.25\n",
            "  Std: 15.83\n",
            "\n",
            "üíæ Enhanced submission saved as 'improved_submission.csv'\n",
            "üìà Submission shape: (28512, 2)\n",
            "\n",
            "üîç Sample predictions:\n",
            "           id     sales\n",
            "1684  3000888  2.250503\n",
            "1685  3002670  2.250503\n",
            "1686  3004452  2.250503\n",
            "1687  3006234  2.250503\n",
            "1688  3008016  2.250503\n",
            "1689  3009798  2.250503\n",
            "1690  3011580  2.250503\n",
            "1691  3013362  2.250503\n",
            "1692  3015144  2.250503\n",
            "1693  3016926  2.250503\n",
            "\n",
            "üìù Original submission not found for comparison\n",
            "\n",
            "‚úÖ IMPROVED MACHINE LEARNING PIPELINE COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "üöÄ KEY IMPROVEMENTS IMPLEMENTED:\n",
            "  ‚úì Data leakage prevention in lag features\n",
            "  ‚úì Proper time series cross-validation\n",
            "  ‚úì Advanced feature engineering (seasonality, interactions)\n",
            "  ‚úì Enhanced models (LightGBM, optimized XGBoost)\n",
            "  ‚úì Robust missing value handling\n",
            "  ‚úì Ensemble predictions\n",
            "  ‚úì Comprehensive evaluation metrics\n",
            "\n",
            "üßπ Memory cleanup completed\n"
          ]
        }
      ],
      "source": [
        "# Model Comparison and Final Predictions\n",
        "print(\"=\"*70)\n",
        "print(\"üèÜ FINAL MODEL COMPARISON AND PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"\\nüìä Cross-Validation Results Summary:\")\n",
        "print(\"-\" * 60)\n",
        "results_df = pd.DataFrame(trainer.results)\n",
        "results_df = results_df.sort_values('mean_rmsle')\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Get best model\n",
        "best_model_name = trainer.get_best_model()\n",
        "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
        "print(f\"   RMSLE: {results_df.iloc[0]['mean_rmsle']:.4f} (+/- {results_df.iloc[0]['std_rmsle']:.4f})\")\n",
        "\n",
        "# Feature importance analysis\n",
        "print(f\"\\nüîç Top 15 Most Important Features ({best_model_name}):\")\n",
        "if best_model_name in trainer.feature_importance:\n",
        "    top_features = trainer.feature_importance[best_model_name].head(15)\n",
        "    print(top_features.to_string(index=False))\n",
        "\n",
        "# Generate ensemble predictions (average of top 2 models)\n",
        "print(\"\\nüéØ Generating Enhanced Predictions...\")\n",
        "\n",
        "if len(trainer.results) >= 2:\n",
        "    # Get top 2 models\n",
        "    top_2_models = results_df.head(2)['model'].tolist()\n",
        "    print(f\"üìà Creating ensemble of: {', '.join(top_2_models)}\")\n",
        "    \n",
        "    # Generate predictions for each model\n",
        "    predictions_dict = {}\n",
        "    for model_name in top_2_models:\n",
        "        pred = trainer.make_predictions(model_name)\n",
        "        predictions_dict[model_name] = pred\n",
        "    \n",
        "    # Weighted ensemble (better model gets higher weight)\n",
        "    weights = [0.7, 0.3]  # Weight based on performance\n",
        "    ensemble_pred = np.zeros_like(predictions_dict[top_2_models[0]])\n",
        "    \n",
        "    for i, model_name in enumerate(top_2_models):\n",
        "        ensemble_pred += weights[i] * predictions_dict[model_name]\n",
        "        print(f\"  {model_name}: weight {weights[i]}\")\n",
        "    \n",
        "    final_predictions = ensemble_pred\n",
        "    prediction_method = \"Ensemble\"\n",
        "    \n",
        "else:\n",
        "    # Use single best model\n",
        "    final_predictions = trainer.make_predictions(best_model_name)\n",
        "    prediction_method = best_model_name\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "final_predictions = np.maximum(final_predictions, 0)\n",
        "\n",
        "print(f\"\\nüìä Final Predictions Summary ({prediction_method}):\")\n",
        "print(f\"  Min: {final_predictions.min():.2f}\")\n",
        "print(f\"  Max: {final_predictions.max():.2f}\")\n",
        "print(f\"  Mean: {final_predictions.mean():.2f}\")\n",
        "print(f\"  Median: {np.median(final_predictions):.2f}\")\n",
        "print(f\"  Std: {final_predictions.std():.2f}\")\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_enhanced['id'],\n",
        "    'sales': final_predictions\n",
        "})\n",
        "\n",
        "# Save enhanced submission\n",
        "submission_filename = 'improved_submission.csv'\n",
        "submission.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Enhanced submission saved as '{submission_filename}'\")\n",
        "print(f\"üìà Submission shape: {submission.shape}\")\n",
        "\n",
        "# Display sample predictions\n",
        "print(f\"\\nüîç Sample predictions:\")\n",
        "sample_size = min(10, len(submission))\n",
        "print(submission.head(sample_size))\n",
        "\n",
        "# Compare with original submission if exists\n",
        "try:\n",
        "    original_submission = pd.read_csv('submission.csv')\n",
        "    \n",
        "    print(f\"\\nüìä Comparison with Original Model:\")\n",
        "    print(f\"  Original mean sales: {original_submission['sales'].mean():.2f}\")\n",
        "    print(f\"  Improved mean sales: {submission['sales'].mean():.2f}\")\n",
        "    print(f\"  Difference: {submission['sales'].mean() - original_submission['sales'].mean():.2f}\")\n",
        "    \n",
        "    # Correlation between predictions\n",
        "    correlation = np.corrcoef(original_submission['sales'], submission['sales'])[0, 1]\n",
        "    print(f\"  Correlation: {correlation:.4f}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"\\nüìù Original submission not found for comparison\")\n",
        "\n",
        "print(f\"\\n‚úÖ IMPROVED MACHINE LEARNING PIPELINE COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Summary of improvements\n",
        "print(f\"\\nüöÄ KEY IMPROVEMENTS IMPLEMENTED:\")\n",
        "print(\"  ‚úì Data leakage prevention in lag features\")\n",
        "print(\"  ‚úì Proper time series cross-validation\")  \n",
        "print(\"  ‚úì Advanced feature engineering (seasonality, interactions)\")\n",
        "print(\"  ‚úì Enhanced models (LightGBM, optimized XGBoost)\")\n",
        "print(\"  ‚úì Robust missing value handling\")\n",
        "print(\"  ‚úì Ensemble predictions\")\n",
        "print(\"  ‚úì Comprehensive evaluation metrics\")\n",
        "\n",
        "# Memory cleanup\n",
        "import gc\n",
        "gc.collect()\n",
        "print(f\"\\nüßπ Memory cleanup completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "8seneca",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
